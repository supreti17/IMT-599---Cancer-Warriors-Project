{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d78ffd3",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Step 1: Export images from QuPath and setup directories](#step-1-export-images-from-qupath-and-setup-directories)\n",
    "2. [Step 2: Setup environment to run YOLO model](#step-2-setup-environment-to-run-yolo-model)\n",
    "3. [Step 3: Convert image masks to labels](#step-3-convert-image-masks-to-labels)\n",
    "4. [Step 4: Validate created labels through visualization](#step-4-validate-created-labels-through-visualization)\n",
    "5. [Step 5: Train model](#step-5-train-model)\n",
    "6. [Step 6: Validate trained model](#step-6-validate-trained-model)\n",
    "7. [Step 7: Repeat steps 5-6 with different versions of YOLO model and parameters until the requirements are met](#step-7-repeat-steps-5-6-with-different-versions-of-yolo-model-and-parameters-until-the-requirements-are-met)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511de9d1-cfdc-4684-8eb8-acc191db98eb",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "### Step 1: Export images from QuPath and setup directories:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506f93c-c7a0-4736-9578-b889c5bcd92f",
   "metadata": {},
   "source": [
    "- Export images from QuPath Using 'export_image_annotations_v2.groovy' script located in this folder.\n",
    "- Once all images are exported:\n",
    "    - Split the images into training and test sets, and move the images into 'train' and 'val' (for test) folders.\n",
    "    - Inside both 'train' and 'val' folders, create three new folders:\n",
    "        - images\n",
    "        - masks\n",
    "        - labels (will be empty for now)\n",
    "\n",
    "    - Move the images (.jpg) into 'images' folder.\n",
    "    - Move the mask (.png) files into 'masks' folder.\n",
    "    - Folder 'labels' will be empty for now. Labels will be create in the next step.\n",
    "    \n",
    "- Directory file structure should look like the following:\n",
    "    - root:\n",
    "        - main.ipynb (this file)\n",
    "        - datasets\n",
    "            - seg\n",
    "                - train\n",
    "                    - images\n",
    "                        - 1.jpg\n",
    "                    - masks\n",
    "                        - 1.png\n",
    "                    - labels\n",
    "                        - 1.txt (will be created in the next step)\n",
    "                - val\n",
    "                    - images\n",
    "                        - 2.jpg\n",
    "                    - masks\n",
    "                        - 2.png\n",
    "                    - labels\n",
    "                        - 2.txt (will be created in the next step)\n",
    "\n",
    "- Now, we are ready to move on to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380db8c8-1cbe-481c-b3f9-d11f9c556d69",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "### Step 2: Setup environment to run YOLO model:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e868aea-8dc5-4d39-a027-289d220f974c",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "#### A: Install required packages to run this notebook:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31549e-60e3-4230-8028-15c9902a19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedf4b9-5bfb-4af5-8510-7e509d8f09be",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "#### B: Create config file:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fddc7f42-8722-43e8-a607-028be9d6693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config file 'config.yaml' with following contents [Remove Hash]:\n",
    "\n",
    "#path: ./seg/\n",
    "#train: train/images/\n",
    "#val: val/images/\n",
    "\n",
    "#nc: 1  # number of classes\n",
    "#names: [ 'tumor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f37af-4fdc-40e7-91f3-a4de0716fc24",
   "metadata": {},
   "source": [
    "### Step 3: Convert image masks to labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7344c-6058-4d62-9c5f-4ad6a4379bd3",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "#### A: Setup Functions to convert masked images to YOLO segmentation format:\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "Functions:\n",
    "1. find_polygons_in_binary_mask(...) - Finds and returns a polygon in a masked image\n",
    "3. parse_segmentation_to_yolo_format(...) - Converts a masked image to YOLO segementation label txt file\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9587c63-40fe-43d1-bee1-1caca9a72cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def find_polygons_in_binary_mask(binary_mask: np.ndarray) -> list[np.ndarray]:\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    polygons = [polygon.squeeze() for polygon in contours]\n",
    "    return polygons\n",
    "\n",
    "def parse_segmentation_to_yolo_format(mask: np.ndarray, background: int | None = None, contour: int | None = None) -> list[list[int | float]]:\n",
    "    \"\"\" Return list of YOLO like annotations for segmentation purposes, i.e. \n",
    "    [\n",
    "        [class_id, x1, y1, x2, y2, ..., xn, yn], # object_0\n",
    "        [class_id, x1, y1, x2, y2, ..., xn, yn], # object_1\n",
    "        ...,\n",
    "        [class_id, x1, y1, x2, y2, ..., xn, yn] # object_m\n",
    "    ]\n",
    "    \"\"\"\n",
    "    unique_label_ids = np.unique(mask).tolist()\n",
    "    if background is not None:\n",
    "        unique_label_ids.remove(background) # remove background from labels\n",
    "    if contour is not None:\n",
    "        unique_label_ids.remove(contour) # remove contour from labels\n",
    "    annotations = []\n",
    "    for label_id in unique_label_ids:\n",
    "        id = 0\n",
    "        binary_mask = ((mask == label_id) * 1).astype(np.uint8)\n",
    "        polygons = find_polygons_in_binary_mask(binary_mask)\n",
    "        wh = np.flip(np.array(binary_mask.shape)) # for normalization purposes\n",
    "        norm_polygons = [polygon / wh for polygon in polygons]\n",
    "        xy_sequences = [polygon.flatten().tolist() for polygon in norm_polygons]\n",
    "        for xy_sequence in xy_sequences:\n",
    "            annotations.append([id] + xy_sequence)\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54667d5e-c820-4398-be0b-587a3b361dd8",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "#### B: Start conversion of masked images to YOLO segmentation label file:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672dea1e-0509-4053-971d-dd1373b548d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def process_mask(mask_file, mask_path):\n",
    "    raw_mask = Image.open(mask_image)\n",
    "    mask = np.array(raw_mask)\n",
    "    print(mask_file)\n",
    "    annotations = parse_segmentation_to_yolo_format(mask, background=0, contour=None)\n",
    "    output_label_path = os.path.splitext(os.path.basename(mask_file))[0] + '.txt'\n",
    "    if not os.path.exists(mask_path + \"labels/\"):\n",
    "        os.mkdir(mask_path + \"labels/\")\n",
    "    with open(mask_path + \"labels/\" + output_label_path, \"w\") as f:\n",
    "        f.write(' '.join(str(x) for x in annotations[0]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for keyword in [\"train\", \"val\"]:\n",
    "        mask_path = \"datasets/seg/{}/\".format(keyword)\n",
    "        for mask_image in glob.glob(mask_path + \"masks/\" + \"*.png\"):\n",
    "            process_mask(mask_image, mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbb626-975e-408e-9f9f-1f60bb6a6d2a",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "### Step 4: Validate created labels through visualization:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2499526-482b-47ea-9c2e-19e7ab77f72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.028] global loadsave.cpp:241 findDecoder imread_('datasets/seg/train/images/NP_MIT_0004_A4.27 [d=1.76228,x=38573,y=36543,w=1128,h=1127].jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/seg/train/labels/NP_MIT_0004_A4.27 [d=1.76228,x=38573,y=36543,w=1128,h=1127].txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNP_MIT_0004_A4.27 [d=1.76228,x=38573,y=36543,w=1128,h=1127]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m img_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(img_path0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m img_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m     segment \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(x\u001b[38;5;241m.\u001b[39msplit(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x)]\n\u001b[1;32m     11\u001b[0m h, w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/seg/train/labels/NP_MIT_0004_A4.27 [d=1.76228,x=38573,y=36543,w=1128,h=1127].txt'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_path0 = \"datasets/seg/train/\"\n",
    "img_name = \"NP_MIT_0004_A4.27 [d=1.76228,x=38573,y=36543,w=1128,h=1127]\"\n",
    "\n",
    "image = cv2.imread(img_path0 + \"images/\" + img_name + \".jpg\")\n",
    "\n",
    "with open(img_path0 + \"labels/\" + img_name + \".txt\") as f:\n",
    "    segment = [np.array(x.split(), dtype=np.float32)[1:].reshape(-1, 2) for x in f.read().strip().splitlines() if len(x)]\n",
    "h, w = image.shape[:2]\n",
    "for s in segment:\n",
    "    s[:, 0] *= w\n",
    "    s[:, 1] *= h\n",
    "cv2.drawContours(image, [s.astype(np.int32) for s in segment], -1, (0, 0, 255), thickness=2)\n",
    "cv2.imwrite(\"1.jpg\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb453ae5-83e5-4a36-9388-a0bf66baa9b3",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "### Step 5: Train model:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ad01f-0ac1-461c-8152-7f7bbbfcc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Train a custom model\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "model.train(data=\"data_seg_config.yaml\", epochs=100, imgsz=640, batch=4, close_mosaic=0, project='bd', flipud=0.5, mosaic=0.5)\n",
    "\n",
    "# OR\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.train(data=\"data_seg_config.yaml\", epochs=300, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daef02-ecd6-4bc1-9642-b074865283a6",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "### Step 6: Validate trained model:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119130b3-cd8b-4423-a5bc-c9c9317c6637",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "#### Validate using predict function:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9ff8a-4778-4975-a355-e4b5d06e24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "trainedModel = YOLO(\"runs/segment/train/weights/best.pt\")\n",
    "trainedModel.predict(source=\"image.png\", save=False, imgsz=[4000,3000]) #save_txt=True) #conf=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d0ce3-5c05-48cb-83d5-432ca9b24cca",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "#### Validate using SAHI inference:\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bf0ea-bfca-4129-9eb3-e403bd58afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.file import download_from_url\n",
    "\n",
    "base_path = \"predict/\"\n",
    "yolov8_model_path = base_path + \"input/trained_model.pt\"\n",
    "img_path = base_path + \"input/image_to_predict.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f885f34-d399-4dd5-bdad-a74657891f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi import AutoDetectionModel\n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",\n",
    "    model_path=yolov8_model_path,\n",
    "    confidence_threshold=0.1,\n",
    "    device=\"cpu\",  # or 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45f796-16e2-42a2-baf3-37e8a47d0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "result = get_sliced_prediction(\n",
    "    img_path,\n",
    "    detection_model,\n",
    "    slice_height=128,\n",
    "    slice_width=128,\n",
    "    overlap_height_ratio=0.2,\n",
    "    overlap_width_ratio=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9360e-fbd4-4165-919f-c270bb5000eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.export_visuals(export_dir=base_path + \"output/\")\n",
    "Image.open(base_path + \"output/predicted_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048611d-e09f-4ac4-83eb-600010a6fd1c",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------\n",
    "### Step 7: Repeat steps 5-6 with different versions of YOLO model and parameters until the requirements are met.\n",
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206a22f-7d0a-4c19-95d3-9d81dfbc7c4b",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------\n",
    "### Findings and Recommendations:\n",
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17f610",
   "metadata": {},
   "source": [
    "In Step 1, we exported the images using the default resolution of 1960 x 1200 as specified in the script. We encountered an issue with exporting annotated images because they were not categorized into Tumor or other classifications. Consequently, the export script generated zero annotated images. To resolve this, we classified all annotations as Tumor in QuPath and saved the images before re-running the export script. This adjustment produced the expected annotated images and masks.\n",
    "\n",
    "In Step 2, after setting up the necessary environments, Step 3 involved converting the image masks (.png) into YOLO label format (.txt) files. We experimented with various conversion scripts and selected the most effective one. Validation of the labels, as detailed in Step 4, was crucial.\n",
    "\n",
    "After validating the images and labels, we proceeded to model training in Step 5. We tested different hyperparameters and YOLOv8 versions (v8n, v8x, v8n-seg, v8n-p2, etc.). Initially, the model failed to detect cells effectively. However, after numerous trials, we discovered that lowering the image resolution in Step 1 and adjusting the epochs to 100-300 improved accuracy and detection rates. Due to limited computing resources, we used the paid version of Google Colab (Pro) for training.\n",
    "\n",
    "Once the model was trained, we validated its performance using the prediction function. Although it detected some cells, it missed others. To enhance detection, we applied SAHI inference (Step 6), which improves detection by dividing the image into smaller sections, processing each section, and then reconstructing the full image with enhanced predictions.\n",
    "\n",
    "Repeat Steps 5 and 6 with different parameters and models as needed until deployment readiness. Due to time constraints and an incomplete model, we could not progress further. To improve the model, consider the following recommendations:\n",
    "\n",
    "- Increase Input Data: Ultralytics suggests using at least 1500 images and 10,000 annotations per class.\n",
    "- Explore Newer Models: Test newer YOLO versions (v9, v10) and other models like Mask R-CNN or RetinaNet.\n",
    "- Use Polygon Annotations: Consider models compatible with polygon-shaped annotations for improved accuracy. Mask R-CNN supports polygons and may offer better performance than YOLO, which converts polygons into rectangles.\n",
    "- Utilize Cloud Services: For more computing power, use platforms like Google Colab Pro or Kaggle, which can significantly speed up the training process, especially under tight deadlines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
